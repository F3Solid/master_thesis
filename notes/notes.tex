\documentclass[12pt,a4paper]{article}
\usepackage{graphicx} % images
\usepackage[acronym]{glossaries} % Acronyms
\usepackage{hyperref} % Hyperlinks
\usepackage{physics} % Physics symbols
\usepackage{showlabels} % Show labels

\newacronym{lrt}{LRT}{Likelihood Ratio Test}

\title{Project Notes}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
The core questions are:
\begin{enumerate}
    \item \textit{\textbf{How well do we need to measure} CBC parameters to infer the CBC population properties at a given \textbf{target redshift} with sufficient precision to learn conclusively if and how they differ from what we observe at redshift \(\sim 0\)?}
    \item \textit{What is the "\textbf{target redshift}"?}
\end{enumerate}

\section{Statistical tests}
To begin with, we can start answering the following question:
\noindent \textit{What kind of observations do we need to conclude that the merger rate density in two mass-redshift bins is different?}

The idea is to use a statistical test to directly compare the number of events in two mass-redshift bins.

\subsection{\acrfull{lrt}}
\begin{itemize}
    \item The statistic in each bin is poissonian distributed: \(\mathrm{Pois}(N, \lambda) = \dfrac{\lambda^N \exp(-\lambda)}{N!}\).
    \item My likelihood is \(\mathcal{L}(\lambda_1, \lambda_2) = \text{Pois}(\lambda_1) \times \text{Pois}(\lambda_2)\), where \(\lambda_1 = N_1\) and \(\lambda_2 = N_2\).
    \item \textbf{Define} \(\alpha = \dfrac{N_1}{N_2} = \dfrac{\lambda_1}{\lambda_2}\): \(\mathcal{L}(\alpha, \lambda_2) = \dfrac{(\alpha \lambda_2)^{N_1}}{N_1!} \exp(-\alpha \lambda_2) \dfrac{\lambda_2^{N_2}}{N_2!} \exp(-\lambda_2)\).
    \item Denote \(\Theta = \{\alpha, \lambda_2\}\) the parameter space. The likelihood-ratio test statistic for the null hypothesis \(H_0 : \theta \in \Theta_0\) is given by: \(\lambda_\text{LR} = -2 \ln\qty[\dfrac{\sup_{\theta \in \Theta_0} \mathcal{L}(\theta)}{\sup_{\theta \in \Theta} \mathcal{L}(\theta)}]\), where \(\Theta_0 = \{\alpha = \alpha_0, \lambda_2\}\). \(\lambda_\text{LR}\) is asymptotically \(\chi^2\) distributed.
    \item It turns out that \begin{equation} \label{eq:lambdaLR}
        \lambda_\text{LR} = 2 N_2 \qty[\alpha\ln\qty(\dfrac{\alpha}{\alpha_0}) - \qty(1 + \alpha) \ln\qty(\dfrac{1 + \alpha}{1 + \alpha_0})]
    \end{equation}
    \item In our case we have two parameters, one of which is fixed \(\Longrightarrow\) \(1\) degree of freedom. If \(\text{p-value} = P(\chi^2_1 > \lambda_\text{LR}) < \text{c.l.} \Longrightarrow \alpha \neq \alpha_0\)
    \item The equation for \(\lambda_\text{LR}\) can be rewritten as: \begin{equation} \label{eq:lambdaLR2}
        \xi \alpha_0^{\dfrac{\alpha}{1 + \alpha}} - \alpha_0 - 1 = 0
    \end{equation} with \begin{equation} \label{eq:xi}
        \xi = \qty[\exp\qty(\dfrac{\lambda_\text{LR}}{2N_2}) \qty(1 + \dfrac{1}{\alpha})^\alpha (1 + \alpha)]^{\dfrac{1}{1 + \alpha}}
    \end{equation}.
    \item If I want to be sure that I've measured \(\alpha \neq \alpha_0\) I need my \(\text{p-value}\) smaller than some confidence level (\(\text{c.l.}\)). Suppose \(\text{c.l.} = 0.05 \Longrightarrow \lambda_\text{LR} \simeq 3.841\). Then I can solve the equation above for \(\alpha\), for a fixed \(\alpha_0\), as a function of \(N_2\).
\end{itemize}

\subsection{Binomial test}
\begin{itemize}
    \item Use the fact that if two independent variables \(X_1\) and \(X_2\) are poissonian distributed with \(/\lambda_1\) and $\lambda_2$, then the distribution of \(X_1\) conditioned on \(X_1 + X_2\) is a binomial distribution with \(p = \dfrac{\lambda_1}{\lambda_1 + \lambda_2}\) and \(k = X_1 + X_2\).
    \item \textbf{Define} \(N_1/N_2 = \lambda_1/\lambda_2 = \alpha\).
    \item \textbf{Then} \(p = \dfrac{\alpha}{1+\alpha}\) and \(p(N_1 | N_1 + N_2) = \mathrm{Bin}(\dfrac{\alpha}{1+\alpha}, N_1 + N_2)\).
    \item The cumulative probability is given by: \begin{equation} \label{eq:binomial}
        P(N_1 \geq N_\ast | N_1 + N_2) = \dfrac{1}{(1+\alpha)^{N_1 + N_2}} \sum_{k=N_\ast}^{N_1 + N_2} {{N_1 + N_2}\choose k} \alpha^k \text{.}
    \end{equation}
    \item This test has the advantage of being exact regardless of the number of measured events in each bin.
\end{itemize}

\subsection{Bayesian approach}
\begin{itemize}
    \item The number of events inside each bin are poissonian distributed as \(\mathrm{Pois}(N_1 | \lambda_1)\) and \(\mathrm{Pois}(N_2 | \lambda_2)\). We are interested in the posterior distribution of the parameters \(\lambda_1\) and \(\lambda_2\).
    \item The likelihood is given by \(\mathcal{L}(N_1, N_2 | \lambda_1, \lambda_2) = \mathrm{Pois}(N_1 | \lambda_1) \mathrm{Pois}(N_2 | \lambda_2)\).
    \item We need to choose a prior for $\lambda_1$ and $\lambda_2$. A convenient choice is the conjugate prior of the Poisson distribution: the Gamma distribution: \begin{equation} \label{eq:Gamma_dist}
        p(\lambda) = \mathrm{Gamma}(\lambda | \alpha, \beta) = \dfrac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} \exp(-\beta\lambda) \text{.}
    \end{equation}
    \item In general the \textbf{posterior} is given by \(p(\lambda_1, \lambda_2 | N_1, N_2) \propto \mathcal{L}(N_1, N_2 | \lambda_1, \lambda_2) p(\lambda_1) p(\lambda_2)\), but using the fact that bins and priors are independent and thanks to conjugate priors we have that \begin{equation} \label{eq:joint_posterior}
        p(\lambda_1, \lambda_2 | N_1, N_2) = p(\lambda_1 | N_1) p(\lambda_2 | N_2) \text{,}
    \end{equation} where \begin{equation} \label{eq:posterior_lambda}
    p(\lambda | N) = \mathrm{Gamma}(\lambda | \alpha + N, \beta + 1) \text{.}
    \end{equation}
    For generic priors we might use MCMC sampling.
    \item A possibile choice for the parameters of the prior distribution might be the (uninformative) \textit{Jeffreys prior}: \(\alpha = 0.5\) and \(\beta = 0\).
    \item Now we can get the \textbf{posterior} of the \textbf{rates} inside each bin using \(R(\Delta m_i, \Delta z_j) = \dfrac{\lambda_{ij}}{\langle VT \rangle_{ij}}\), where $\lambda_{ij}$ are samples from the posterior of the bin.
    \item Most importantly we can get the \textbf{posterior} of rates \textbf{ratios}: \(\dfrac{R_1}{R_2} = \dfrac{\lambda_1}{\lambda_2}\dfrac{\dfrac{1}{\left\langle VT \right\rangle_1}}{\dfrac{1}{\left\langle VT \right\rangle_2}}\).
\end{itemize}

\subsubsection{How to compute \(\langle VT \rangle\)?}
\begin{itemize}
    \item \textbf{Simulated injections} from a given population model \(p(\theta)\).
    \item \textbf{Data driven} method assuming \(p_i(\theta) = \delta(\theta - \theta_i)\), where \(\theta_i\) are observed events: \begin{equation} \label{eq:VT_point}
        \langle VT \rangle = \dfrac{N_\text{obs}}{\hat{R}} = \dfrac{N_\text{obs} V_\text{bin} T_\text{obs}}{\sum_k^{N_\text{obs}} \dfrac{1+z_k}{p_{\text{det}, k}}}
    \end{equation}
\end{itemize}

\section{Connection with the rates}
We want to connect the number of events in a bin with the merger rate density in that bin. We have two ways to do that:
\begin{itemize}
    \item \begin{equation} \label{eq:rate}
        R(\Delta m_i, \Delta z_j) = N_\text{obs}^{ij} / \langle VT \rangle_{ij} \text{,}
    \end{equation} with \begin{equation} \label{eq:VT}
        \langle VT \rangle_{ij} = T_\text{obs} \int_{\Delta m_i, \Delta z_j} \dfrac{1}{1+z} \dfrac{dV_c}{dz} p(\theta)p_\text{det}(m, \theta, z) dm dz d\theta \text{.}
    \end{equation}
    \item \begin{equation} \label{eq:rate2}
        R(\Delta m_i, \Delta z_j) = \dfrac{1}{\Delta V_j T_\text{obs}^{ij}} \sum_k^{N_{ij}} \dfrac{1 + z_k}{p_\text{det}(m_k, \theta_k, z_k)}
    \end{equation} where \(\Delta V_j = \int_{\Delta z_j} dz \dfrac{dV_c}{dz}\). In this case we can also get the variance of the rate by squaring the argument of the sum in \eqref{eq:rate2}.
\end{itemize}

\end{document}